{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for analyzing array data from a specific project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%matplotlib inline\n",
    "import sys as sys\n",
    "sys.path.append('../Utils')\n",
    "from IPython.display import display, HTML\n",
    "from __future__ import division, print_function\n",
    "from scipy import io\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import hclusterplot as hcp\n",
    "import myboxplot as mbp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import statsmodels.api as sm\n",
    "import amplotlib as amp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set all project specific parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ROOT_PATH = '/Users/thertz/Dropbox/HertzLab/'\n",
    "project_name = 'H7N9';\n",
    "pathogen_name = 'Influenza'\n",
    "\n",
    "# Dates of experiments, each one is a directory where GPR files are saved.\n",
    "# multiple dates can be entered, but must have a corresponding prefix\n",
    "experiment_dates = ['06_03_2015'] #['08_21_2014', '08_22_2014', '08_25_2014', '09_04_2014', '09_05_2014']; \n",
    "\n",
    "# names of HA and NA proteins that are on the array. \n",
    "prot_names = ['SHA_ha', 'SHA_na', 'Cal_ha', 'Cal_na']\n",
    "# the names for figure plotting\n",
    "prot_strs = ['H7', 'N9', 'H1', 'N1']\n",
    "\n",
    "# labels of experimental groups\n",
    "exp_groups = ['Normal', 'Obese']\n",
    "exp_group_prefixes = ['Obese_', 'Normal_']\n",
    "\n",
    "# summary statistics of the array - breadth and magnitude\n",
    "arr_summary_stats = ['H7_mag', 'N9_mag', 'H1_mag', 'N1_mag']\n",
    "\n",
    "save_path = os.path.join(ROOT_PATH, 'ArrayData', pathogen_name, project_name)\n",
    "load_path = save_path\n",
    "fig_path = os.path.join(ROOT_PATH, 'ArrayData', pathogen_name, project_name, 'Figs')\n",
    "\n",
    "# all slides and slideToSampleMapping file for each date have the exact same filePrefix\n",
    "exp_prefix = [None] * len(experiment_dates) # initalize empty list of size experiment_dates\n",
    "for i, exp in enumerate(experiment_dates):\n",
    "  exp_str = exp.replace('20', '') # strip away the 20 from the year part of the date\n",
    "  exp_prefix[i] = \"\".join([project_name, '_', exp_str])\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Additional parameters - project specific\n",
    "#-------------------------------------------------------------------------------\n",
    "num_arrays = 2;  # number of arrays on each slide.\n",
    "type_flag  = 'median' # uses the median over replicates of an antigen. Can also be 'mean'\n",
    "color_flags = ['635']\n",
    "color_tags = ['IgG']\n",
    "font_size = 12\n",
    "\n",
    "y_lims = [0, 65000] # max height of y in graphs\n",
    "y_lims_summary = [0, 32500] # max height of y in summary stat graphs\n",
    "min_threshold = 10000 # minimal threshold for peak responses, used by findPeaks.\n",
    "bg_threshold = 5000 # threshold by which to flag antigens as ones that have high background\n",
    "summary_method = 'median' # summary stat used for group comparisons, and plotting. \n",
    "\n",
    "# Unsupervised clustering parameters:\n",
    "num_clusters = 4 # number of clusters in the data (or expected number)\n",
    "minResponseThreshold = 2000 # minimal threshold for responces to be included in clusterSamplesByResponseVectors, point below data is noise\n",
    "\n",
    "#-------------------------------------------------------------------------------#\n",
    "# Stat comparison parameters: (also project specific)\n",
    "#-------------------------------------------------------------------------------#\n",
    "# Filters used for treatment-blinded filtering of responses to single antigens used for statistical analysis.\n",
    "# currently only one filter is implemented which is # responders within the entire treatment set (ignoring all controls) \n",
    "filterNames = ['PercentResponders']\n",
    "filterThresholds = [0.2]\n",
    "treatmentGroups = ['Ob_post_Vac', 'WT_post_Vac'] # Names of the two groups that are to be compared statistically below (currently supports only pairwise comparisons)\n",
    "\n",
    "# data Labels\n",
    "maskLabels = ['Cal_HA_Inds', 'Cal_NA_Inds', 'Sha_HA_Inds', 'Sha_NA_Inds']\n",
    "treatmentLabels = ['WT-pre-vac', 'obese-pre-vac', 'wt-post-vac', 'obese-post-vac']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data from matlab mat files\n",
    "This is obsolete and will be modified in future version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/thertz/Dropbox/HertzLab/ArrayData/Influenza/H7N9/06_03_2015/H7N9_06_03_15_arrayData_median.mat\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------#\n",
    "# Read in all mat files of array data, and strip them out from the matstructs \n",
    "# into a dataframe:\n",
    "#-------------------------------------------------------------------------------#\n",
    "arr_df = None\n",
    "for i, exp in enumerate(experiment_dates):\n",
    "\n",
    "    mapping_filename = os.path.join(load_path, exp, \"\".join([exp_prefix[i], 'SlideToSampleMappings.txt'])) \n",
    "    array_data_filename = os.path.join(save_path, exp, \"\".join([exp_prefix[i], '_arrayData_', type_flag, '.mat']))\n",
    "    print(array_data_filename)\n",
    "    \n",
    "    d = io.loadmat(array_data_filename, struct_as_record=False)['arrayData']\n",
    "    matstruct = d[0][0]\n",
    "    ptids = [matstruct.ptids[0][i][0] for i in np.arange(matstruct.ptids[0].shape[0])]\n",
    "    group_names = [matstruct.groupNames[0][i][0] for i in np.arange(matstruct.ptids[0].shape[0])]\n",
    "    \n",
    "    res = matstruct.responseMatrix[0][0]\n",
    "    antigens = [matstruct.antigenNames[i][0][0] for i in np.arange(matstruct.antigenNames.shape[0])]\n",
    "\n",
    "    columns = antigens + ['group']\n",
    "    data = np.column_stack((res, np.asarray(group_names)))\n",
    "\n",
    "    if arr_df is None:\n",
    "        arr_df = pd.DataFrame(data, index=ptids, columns=columns)\n",
    "    else:\n",
    "        arr_df = pd.concat((arr_df, pd.DataFrame(data, index=ptids, columns=columns)), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background subtraction\n",
    "BSA responses are used for background subtraction and the maximal BSA response is subtracted from all antigens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------#\n",
    "# Background subtraction\n",
    "\n",
    "# remove negative control responses as measured by BSA alone:\n",
    "# background subtraction: subtract maximal BSA response for each antigen\n",
    "#-----------------------------------------------------------------#\n",
    "bsa_inds = arr_df.index.to_series().str.contains('BSA')\n",
    "bg_df = arr_df[bsa_inds]\n",
    "if len(bg_df.shape) == 1:\n",
    "    bg_responses = np.asarray(bg_df[antigens])\n",
    "else:\n",
    "    bg_responses = np.asarray(bg_df[antigens].max())\n",
    "fg_inds = ~bsa_inds\n",
    "\n",
    "bg_df = arr_df[bsa_inds]\n",
    "arr_df = arr_df[fg_inds]\n",
    "\n",
    "curr_data = np.array(arr_df.as_matrix(columns=[antigens]), dtype=float) - bg_responses.T\n",
    "curr_data[np.where(curr_data < 0)] = 0\n",
    "arr_df[antigens] = curr_data\n",
    "arr_df[antigens] = arr_df[antigens].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Index and group dictionary setup:\n",
    "Here specific dictionaries for accessing subsets of the array limited to given protein are defined. They are based on the prot_names, prot_strs and exp_groups defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------#\n",
    "# Index and group dictionary setup:\n",
    "\n",
    "# create dictionary with index sets for all HA and NA proteins\n",
    "# These are antigen mask sets...\n",
    "# Note that these can only be defined After creating the \n",
    "# arr_df DataFrame\n",
    "#-----------------------------------------------------------------#\n",
    "ind_dict = {}  # indices of cols for each strain:\n",
    "for p, s in zip(prot_names, prot_strs):\n",
    "    # define indices into columns of all specific HA and NA seqs:\n",
    "    ind_dict[p] = [col for col in arr_df.columns if (col[:6] == p and col[7:].isdigit())]  \n",
    "\n",
    "\n",
    "# create dictionay of all exp groups in the data where data is indices into the arr_df\n",
    "group_inds = {}\n",
    "for e in exp_groups:\n",
    "    group_inds[e] = arr_df.loc[arr_df['group'] == e].index\n",
    "\n",
    "# label dictionary from group to a numeric label\n",
    "group_labels = {}\n",
    "for l, e in enumerate(exp_groups):\n",
    "    group_labels[e] = l\n",
    "\n",
    "# dicionary for time-points, i.e. for pre/post etc.\n",
    "time_dict = {}\n",
    "# if pre/post data:\n",
    "#time_dict['Pre'] = arr_df.group.str.contains('pre')\n",
    "#time_dict['Post'] = arr_df.group.str.contains('post')\n",
    "\n",
    "time_dict['all'] = [True]*arr_df.shape[0] # point to all data if not timepoints\n",
    "\n",
    "\n",
    "# add numeric label column for each group\n",
    "arr_df['group_label'] = arr_df['group'].map(group_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breadth and Magnitude summary statistics of the array data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------#\n",
    "# Breadth and Magnitude summary statistics for array data:\n",
    "#-----------------------------------------------------------------#\n",
    "# compute summary statistics of the array - breadth and magnitude and store in new columns in arr_df\n",
    "# uses the ind_dict from above\n",
    "for p, s in zip(prot_names, prot_strs):\n",
    "    # insert new columns into dataframe for overall magnitude for each strain\n",
    "    arr_df[s + '_mag'] = arr_df[ind_dict[p]].sum(axis=1)  \n",
    "\n",
    "    # breadth - response is positive if it is above the mean response of that antigen across all samples\n",
    "    for col in ind_dict[p]:\n",
    "        arr_df[col + '_binarized'] = arr_df[col].map(lambda s: 1 if s > 2000 else 0)\n",
    "\n",
    "    arr_df[s + '_breadth'] = \\\n",
    "        arr_df[[col for col in arr_df.columns if (col[:6] == p and col.endswith('_binarized'))]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------#\n",
    "# 1. Clustering analysis:\n",
    "#-------------------------------------------------------------------------------#\n",
    "# Cluster using Andrew's package \n",
    "# (complete linkage using Spearman correlation coefficient):\n",
    "\n",
    "dMat = {}  # distance matrices\n",
    "Z_struct = {}  # clustering struct\n",
    "dend = {}  # dendrogram struct\n",
    "clusters = {}  # cluster labels\n",
    "cluster_treatment_stats = {}\n",
    "pred_treatment_labels = {}  # predicted treatment labels based on clustering\n",
    "\n",
    "# cluster a given timepoint (can be Post, Pre, all etc.)\n",
    "post_inds = time_dict['all'] \n",
    "p_labels = np.unique(arr_df[post_inds].group_label.values)\n",
    "\n",
    "# clustering occurs for each protein separately: i.e. H7, N9, H1, etc.\n",
    "for k in ind_dict.keys():\n",
    "    # use Andrew's package which allows clustering using Spearman distances \n",
    "    # (sch.linkage, and pdist do not support this for some reason, unlike Matlab)\n",
    "    (dMat[k], Z_struct[k], dend[k]) = \\\n",
    "        hcp.computeHCluster(arr_df[post_inds][ind_dict[k]], method='complete', metric='spearman')\n",
    "    clusters[k] = sch.fcluster(Z_struct[k], t=num_clusters, criterion='maxclust')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Statistical comparisons between groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------#\n",
    "# 2. Statistical comparisons between groups\n",
    "#-------------------------------------------------------------------------------#\n",
    "  # ranksum tests \n",
    "  #[expGroupStruct(i).totalMagP] = ranksum(totalMagnitude(WT_postInds),totalMagnitude(Ob_postInds));\n",
    "  #[expGroupStruct(i).H7N9magP]  = ranksum(H7N9magnitude(WT_postInds),H7N9magnitude(Ob_postInds));\n",
    "\n",
    "\n",
    "# compute median responses by treatment group:\n",
    "group_medians = {}\n",
    "group_stds = {}\n",
    "for p, s in zip(prot_names, prot_strs):\n",
    "    for g in group_inds.keys():\n",
    "        curr_df = arr_df.loc[group_inds[g]][ind_dict[p]]\n",
    "        group_medians[g, s] = curr_df.apply(np.median, axis=0)\n",
    "        group_stds[g, s] = curr_df.apply(np.std, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
